{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Simple Linear Regression?\n",
    "'''\n",
    "simple linear regression model is supervised machine learning model that uses to predict continous data .\n",
    "in simple linear regression model there is only one independent varibale and one dependent variable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What are the key assumptions of Simple Linear Regression?\n",
    "'''\n",
    "1. it takes only one independent variable and one dependent variable.\n",
    "2. it doesn't handle multiple features .\n",
    "3. it requireds continuous varible.\n",
    "4. it works on olny numeric data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What does the coefficient m represent in the equation Y=mX+c?\n",
    "'''\n",
    "in simple linear regression m represents the slope of line or gradient .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What does the intercept c represent in the equation Y=mX+c?\n",
    "'''\n",
    "in simple linear regression model c is the hight at which the line crosses the y-axis  also known as y-intercept\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.  How do we calculate the slope m in Simple Linear Regression ?\n",
    "'''\n",
    "divide the standard deviation of y values by the standard deviation of x values and then multiply this by the correlation between x and y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. What is the purpose of the least squares method in Simple Linear Regression?\n",
    "''' The least squares method in simple linear regression finds the line of best fit for a set of data points. It does this by minimizing the sum of the squared errors between the data points and the line'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
    "'''\n",
    "n simple linear regression, the coefficient of determination (\\(R^{2}\\)) is a statistical measure of how well a regression line fits the data. It's also known as the \\(R\\)-squared value.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What is Multiple Linear Regression\n",
    "'''\n",
    "multiple linear regression is superviesd machine learning algorithm which is used to predict the continous ouput . we can used it when independent featues are multiple.\n",
    "multiple linear regression is statistical technique that analyze how multiple independent variables are relate to single dependent variable.  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. What is the main difference between Simple and Multiple Linear Regression\n",
    "'''\n",
    "In simple linear regression there is only one dependent and indepedent variable.\n",
    "but in muliple linear reagression there are multiple independent variables.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10. What are the key assumptions of Multiple Linear Regression\n",
    "'''\n",
    "Assumptions: \n",
    "1. NO Multicollinearity\n",
    "2. Linearity\n",
    "3. Independance\n",
    "4. Hetroscadacity\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
    "'''\n",
    "Heteroscedasticity occurs when the variance of the residuals (errors) in a regression model is not constant across all levels of the independent variables. In simpler terms, it means that the spread of the errors differs at different levels of the predicted values. This violates the assumption of homoscedasticity, which is one of the key assumptions of Multiple Linear Regression.\n",
    "1. Incorrect Confidence Intervals\n",
    "2. Inefficiency of Estimators\n",
    "3. Biased Standard Errors\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q12. How can you improve a Multiple Linear Regression model with high multicollinearity\n",
    "'''\n",
    "High multicollinearity can complicate the interpretation of a Multiple Linear Regression (MLR) model and reduce the reliability of the coefficients. Here are some effective ways to improve an MLR model with high multicollinearity:\n",
    "Variance Inflation Factor (VIF)\n",
    "Lasso Regression\n",
    "Ridge Regression\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q13. What are some common techniques for transforming categorical variables for use in regression models\n",
    "'''\n",
    "1. label encoding\n",
    "2. one-hot-encoding\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q14.  What is the role of interaction terms in Multiple Linear Regression\n",
    "'''\n",
    "he primary purpose of interaction terms is to account for the possibility that the effect of one predictor on the dependent variable may depend on the level of another predictor. In other words, interaction terms allow the model to capture non-additive relationships between the predictors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
    "'''\n",
    "simple:ùõΩ0(Intercept): This represents the expected value of ùëåwhen ùëã=0.In other words, it's the value of the dependent variable when the independent variable is zero. The interpretation is straightforward: it gives the baseline value of ùëå in the absence of X\n",
    "\n",
    "multiple: In MLR, the intercept represents the expected value of ùëåwhen all the independent variables (X1,ùëã2,‚Ä¶,ùëãùëù) are equal to zero. The interpretation becomes more complex, as it now assumes that all independent variables are zero simultaneously. This is often unrealistic in practical situations, especially when the variables cannot logically be zero.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q16. What is the significance of the slope in regression analysis, and how does it affect predictions\n",
    "'''\n",
    "Rate of Change: The slope represents the rate of change in the dependent variable for every one-unit change in the independent variable. In simpler terms, it tells you how much Y changes when X increases by one unit.\n",
    "Direction of Relationship: The sign of the slope indicates the direction of the relationship:\n",
    "Positive Slope: If the slope is positive, it means that as X increases, Y also increases. There's a direct relationship between the two variables.\n",
    "Negative Slope: If the slope is negative, it means that as X increases, Y decreases. There's an inverse relationship between the two variables.\n",
    "Magnitude: The magnitude of the slope indicates the strength of the relationship. A larger absolute value means a steeper slope and a stronger relationship, while a smaller absolute value means a gentler slope and a weaker relationship.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q17 How does the intercept in a regression model provide context for the relationship between variables\n",
    "'''\n",
    "it tell us where the regression line cross the y-axis.\n",
    "also intercept help in understanding the starting point of the relationship.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q18.  What are the limitations of using R¬≤ as a sole measure of model performance\n",
    "'''\n",
    "not suitable for non-linear relationship.\n",
    "overfitting - high r-squared indicates overfitting.\n",
    "it is hevily influenced by outliers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q19.  How would you interpret a large standard error for a regression coefficient\n",
    "'''\n",
    "A large standard error suggests that the coefficient estimate is not precise\n",
    "A large standard error leads to a wider confidence interval for the coefficient.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
    "'''\n",
    "if there appears to be a fan or cone shape in the residual plot, it indicates the presence of heteroskedasticity.\n",
    "Identifying and addressing heteroscedasticity is crucial for accurate regression analysis\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤\n",
    "'''\n",
    "means added features are not contributing to increase performance of model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q22.  Why is it important to scale variables in Multiple Linear Regression\n",
    "'''\n",
    "Scaling variables in multiple linear regression is important because it improves the model's efficiency, stability, and interpretability\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q23.  What is polynomial regression\n",
    "'''\n",
    "polynomial regression is a machine learning technique that models the relationship between variables using polynomial equation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q24.  How does polynomial regression differ from linear regression\n",
    "'''\n",
    "linear regression models the linear relationship between variables .\n",
    "and polynomial resgression models the non-linear relationship between varibales.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q25.  When is polynomial regression used\n",
    "'''\n",
    "polynomial regression used when there is no linear corelation between variables.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q26.  What is the general equation for polynomial regression\n",
    "'''\n",
    "Y = a + B1X1 + B2(X2)**2 + B3(X3)**3 +......+Bn(Xn)**n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q27.  Can polynomial regression be applied to multiple variables\n",
    "'''\n",
    "Yes , polynomial regression can be applied to mulitple variables.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q28. What are the limitations of polynomial regression\n",
    "'''\n",
    "Interpretability: Higher-degree polynomial models are less interpretable\n",
    "Sensitivity to Outliers\n",
    "Computational Cost\n",
    "Multicollinearity\n",
    "Overfitting\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q29.  What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
    "'''\n",
    "The solution is to have a separate validation set on which we can evaluate the model for each polynomial degree.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q30.  Why is visualization important in polynomial regression\n",
    "'''\n",
    "Now when we visualize the results of regression polynomial, we can see how well the contour has plotted.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q31.  How is polynomial regression implemented in Python?\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree= 2)\n",
    "#then we fit_transform the polynomial regression on training data and transoform on test data . then we fit the linear regression model on that data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
